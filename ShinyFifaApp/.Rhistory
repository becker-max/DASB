set.seed(99)
set.seed(69)
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Overall","Value","Wage","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve")]
train.data<- sample_frac(fifaDataRaw,0.8)
forecData <- fifaDataRaw[,c("Age","Overall","Value","Wage","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve")]
train.data<- sample_frac(forecData,0.8)
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Overall","Value","Wage","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve")]
train.data<- sample_frac(forecData,0.8)
test.data <- setdiff(forecData,train.data)
train.data
forecData.cor<- cor(forecData)
forecData.cor
forecData.corr<- cor(forecData)
forecData.cor
forecData
forecData.corr<- cor(forecData)
forecData.corr
forecData.corr<- cor(forecData,use = "complete.obs")
forecData.corr
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Overall","Value","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve")]
train.data<- sample_frac(forecData,0.8)
test.data <- setdiff(forecData,train.data)
forecData
forecData.corr<- cor(forecData,use = "complete.obs")
forecData.corr
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Overall","Value","Wage","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve")]
train.data<- sample_frac(forecData,0.8)
test.data <- setdiff(forecData,train.data)
forecData
forecData.corr<- cor(forecData,use = "complete.obs")
forecData.corr
#strating with the most correlated variable as predictor
lm1 <- lm(Value~Wage,data=train.data )
lm1
summary(lm1)
shiny::runApp()
forecData <- fifaDataRaw[,c("Age","Value","Wage","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve","HeightMetric","WeightMetric")]
train.data<- sample_frac(forecData,0.8)
test.data <- setdiff(forecData,train.data)
forecData
forecData.corr<- cor(forecData,use = "complete.obs")
forecData.corr
ggcorr(firecData.corr)
ggcorr(forecData.corr)
ggcorr(forecData.corr,lables=TRUE)
ggcorr(forecData.corr,lable=TRUE)
ggcorr(forecData.corr,label=TRUE)
forecData.corr
forecData.corr$Value
forecData.corr[,c("Value")]
view(forecData.corr[,c("Value")])
#strating with the most correlated variable as predictor
lm1 <- lm(Value~Wage,data=train.data )
lm1
summary(lm1)
view(forecData.corr[,c("Value")])
#strating with the most correlated variable as predictor
#Wage would be the the most correlated. but since wage and value are naturally dependend on each other (since the more value, the more wage) and it would be meaningless to predict ones value from the wage we go from the 2nd highest
lm1 <- lm(Value~ShortPassing,data=train.data )
lm1
summary(lm1)
lm2<-(Value~ShortPassing+Volleys,data=train.data)
lm2<-(Value~ShortPassing+Volleys,data=train.data)
lm2 <- lm(Value~ShortPassing+Volleys,data=train.data)
summary(lm2)
lm3 <- lm(Value~ShortPassing+Volleys+Curve,data=train.data)
summary(lm3)
lm4 <- lm(Value~ShortPassing+Volleys+Curve+Dribbling,data=train.data)
summary(lm4)
lm5 <- lm(Value~ShortPassing+Volleys+Curve+Dribbling+Finishing,data=train.data)
summary(lm5)
lm.tot <- lm(Value~.,data=train.data)
lm.tot <- lm(Value~.,data=train.data)
lm.red <- lm(Value~.-WeightMetric-HeightMetric-Ager,data=train.data)
lm.tot <- lm(Value~.,data=train.data)
lm.red <- lm(Value~.-HeightMetric-WeightMetric-Age,data=train.data)
summary(lm.red)
#let's cpompare the models obtained, in a structured way
anova(lm1, lm2, lm3, lm4,lm5, lm.red , lm.tot )
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Value","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve","HeightMetric","WeightMetric")]
train.data<- sample_frac(forecData,0.7) #select 70% random samples
test.data <- setdiff(forecData,train.data)
forecData
forecData.corr<- cor(forecData,use = "complete.obs")
view(forecData.corr[,c("Value")])
ggcorr(forecData.corr,label=TRUE)
#strating with the most correlated variable as predictor
#Wage would be the the most correlated. but since wage and value are naturally dependend on each other (since the more value, the more wage) and it would be meaningless to predict ones value from the wage we go from the 2nd highest
lm1 <- lm(Value~ShortPassing,data=train.data )
lm1
summary(lm1)
lm2 <- lm(Value~ShortPassing+Volleys,data=train.data)
summary(lm2)
lm3 <- lm(Value~ShortPassing+Volleys+Curve,data=train.data)
summary(lm3)
lm4 <- lm(Value~ShortPassing+Volleys+Curve+Dribbling,data=train.data)
summary(lm4)
lm5 <- lm(Value~ShortPassing+Volleys+Curve+Dribbling+Finishing,data=train.data)
summary(lm5)
#All Data, generally a bad idea due to data noise
lm.tot <- lm(Value~.,data=train.data)
#All Data, generally a bad idea due to data noise
lm.tot <- lm(Value~.,data=train.data)
#All Data minus the least important predictors
lm.red <- lm(Value~.-HeightMetric-WeightMetric-Age,data=train.data)
summary(lm.red)
#let's cpompare the models obtained, in a structured way
anova(lm1, lm2, lm3, lm4,lm5, lm.red , lm.tot )
view(forecData.corr[,c("Value")])
#Conlcusion:
#lm.tot is the best but has way too many parameters to be given in our interactive UI and includes the Wage
#lm.red is the same
#lm4 -> sweet spot for best prediction with the least amount of variables
lm <- lm4
pred1 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve,Dribbling=test.data$Dribbling), interval="prediction")
pred2 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve,Dribbling=test.data$Dribbling), interval="confidence")
pred1.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred1)
pred2.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred2)
pred_test <- predict(lm,data.frame(ShortPassing=1,Volleys=1,Curve= 2,Dribbling=0), interval="prediction")
view(pred_test[1])
pred1 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve,Dribbling=test.data$Dribbling), interval="prediction")
pred2 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve,Dribbling=test.data$Dribbling), interval="confidence")
pred1.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred1)
pred2.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred2)
pred1.df
pred2.df
runApp()
runApp()
view(pred_test)
pred1.df
pred2.df
runApp()
runApp()
runApp()
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Value","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve","HeightMetric","WeightMetric")]
train.data<- sample_frac(forecData,0.7) #select 70% random samples
test.data <- setdiff(forecData,train.data)
forecData
forecData.corr<- cor(forecData,use = "complete.obs")
ggcorr(forecData.corr,label=TRUE)
#strating with the most correlated variable as predictor
#Wage would be the the most correlated. but since wage and value are naturally dependend on each other (since the more value, the more wage) and it would be meaningless to predict ones value from the wage we go from the 2nd highest
lm1 <- lm(Value~ShortPassing,data=train.data )
lm1
summary(lm1)
lm2 <- lm(Value~ShortPassing+Volleys,data=train.data)
summary(lm2)
lm3 <- lm(Value~ShortPassing+Volleys+Curve,data=train.data)
summary(lm3)
lm4 <- lm(Value~ShortPassing+Volleys+Curve+Dribbling,data=train.data)
summary(lm4)
lm5 <- lm(Value~ShortPassing+Volleys+Curve+Dribbling+Finishing,data=train.data)
summary(lm5)
#All Data, generally a bad idea due to data noise
lm.tot <- lm(Value~.,data=train.data)
#All Data minus the least important predictors
lm.red <- lm(Value~.-HeightMetric-WeightMetric-Age,data=train.data)
summary(lm.red)
summary(lm.red)
#let's cpompare the models obtained, in a structured way
anova(lm1, lm2, lm3, lm4,lm5, lm.red , lm.tot )
#Conlcusion:
#lm.tot is the best but has way too many parameters to be given in our interactive UI and includes the Wage
#lm.red is the same as lm.tot
#lm4 -> sweet spot for best prediction with the least amount of variables
lm <- lm4
#Set the predictions
pred1 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve,Dribbling=test.data$Dribbling), interval="prediction")
pred2 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve,Dribbling=test.data$Dribbling), interval="confidence")
pred1.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred1)
pred2.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred2)
pred1.df
pred2.df
pred_test <- predict(lm,data.frame(ShortPassing=1,Volleys=1,Curve= 2,Dribbling=0), interval="prediction")
# view(pred_test)
pred_test[1]
view(forecData.corr[,c("Value")])
view(forecData.corr[,c("Value")])
pred_test <- predict(lm,data.frame(ShortPassing=80,Volleys=70,Curve= 60,Dribbling=40), interval="prediction")
# view(pred_test)
pred_test[1]
runApp()
view(forecData.corr[,c("Value")])
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Value","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve","HeightMetric","WeightMetric")]
train.data<- sample_frac(forecData,0.7) #select 70% random samples
test.data <- setdiff(forecData,train.data)
forecData
forecData.corr<- cor(forecData,use = "complete.obs")
view(forecData.corr[,c("Value")])
ggcorr(forecData.corr,label=TRUE)
#strating with the most correlated variable as predictor
#Wage would be the the most correlated. but since wage and value are naturally dependend on each other (since the more value, the more wage) and it would be meaningless to predict ones value from the wage we go from the 2nd highest
lm1 <- lm(Value~ShortPassing,data=train.data )
lm1
summary(lm1)
lm2 <- lm(Value~ShortPassing+Volleys,data=train.data)
summary(lm2)
lm3 <- lm(Value~ShortPassing+Volleys+Curve,data=train.data)
summary(lm3)
lm4 <- lm(Value~ShortPassing+Volleys+Curve+Dribbling,data=train.data)
summary(lm4)
lm5 <- lm(Value~ShortPassing+Volleys+Curve+Dribbling+Finishing,data=train.data)
summary(lm5)
#All Data, generally a bad idea due to data noise
lm.tot <- lm(Value~.,data=train.data)
#All Data minus the least important predictors
lm.red <- lm(Value~.-HeightMetric-WeightMetric-Age,data=train.data)
summary(lm.red)
#let's cpompare the models obtained, in a structured way
anova(lm1, lm2, lm3, lm4,lm5, lm.red , lm.tot )
runApp()
pred_test <- predict(lm,data.frame(ShortPassing=80,Volleys=70,Curve= 60,Dribbling=40), interval="prediction")
# view(pred_test)
pred_test[1]
pred_test <- predict(lm,data.frame(ShortPassing=80,Volleys=70,Curve= 60,Dribbling=1), interval="prediction")
# view(pred_test)
pred_test[1]
runApp()
view(fifaDataRaw)
pred_test <- predict(lm,data.frame(ShortPassing=80,Volleys=70,Curve= 60,Dribbling=1), interval="prediction")
# view(pred_test)
pred_test[1]
pred_test <- predict(lm,data.frame(ShortPassing=80,Volleys=70,Curve= 60,Dribbling=100), interval="prediction")
# view(pred_test)
pred_test[1]
#let's cpompare the models obtained, in a structured way
anova(lm1, lm2, lm3, lm4,lm5, lm.red , lm.tot )
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Value","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve","HeightMetric","WeightMetric")]
train.data<- sample_frac(forecData,0.8) #select 70% random samples
test.data <- setdiff(forecData,train.data)
forecData
forecData.corr<- cor(forecData,use = "complete.obs")
runApp()
#Conlcusion:
#lm.tot is the best but has way too many parameters to be given in our interactive UI and includes the Wage
#lm.red is the same as lm.tot
#lm4 -> sweet spot for best prediction with the least amount of variables
lm <- lm3
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Value","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve","HeightMetric","WeightMetric")]
train.data<- sample_frac(forecData,0.8) #select 70% random samples
test.data <- setdiff(forecData,train.data)
forecData
forecData.corr<- cor(forecData,use = "complete.obs")
#strating with the most correlated variable as predictor
#Wage would be the the most correlated. but since wage and value are naturally dependend on each other (since the more value, the more wage) and it would be meaningless to predict ones value from the wage we go from the 2nd highest
lm1 <- lm(Value~ShortPassing,data=train.data )
lm1
summary(lm1)
lm2 <- lm(Value~ShortPassing+Volleys,data=train.data)
summary(lm2)
lm3 <- lm(Value~ShortPassing+Volleys+Curve,data=train.data)
summary(lm3)
lm4 <- lm(Value~ShortPassing+Volleys+Curve+Dribbling,data=train.data)
summary(lm4)
lm5 <- lm(Value~ShortPassing+Volleys+Curve+Dribbling+Finishing,data=train.data)
summary(lm5)
#All Data, generally a bad idea due to data noise
lm.tot <- lm(Value~.,data=train.data)
#All Data minus the least important predictors
lm.red <- lm(Value~.-HeightMetric-WeightMetric-Age,data=train.data)
summary(lm.red)
#let's cpompare the models obtained, in a structured way
anova(lm1, lm2, lm3, lm4,lm5, lm.red , lm.tot )
#Set the predictions
pred1 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve), interval="prediction")
pred2 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve), interval="confidence")
pred1.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred1)
pred2.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred2)
pred1.df
pred2.df
pred_test <- predict(lm,data.frame(ShortPassing=80,Volleys=70,Curve= 60), interval="prediction")
# view(pred_test)
pred_test[1]
pred_test <- predict(lm,data.frame(ShortPassing=80,Volleys=70,Curve= 80), interval="prediction")
# view(pred_test)
pred_test[1]
runApp()
pred1.df
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Value","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve","HeightMetric","WeightMetric")]
outliers <- boxplot(forecData$Value, plot = TRUE)$out
x <- forecData
forecData <- x[-which(x$Value %in% outliers),]
train.data<- sample_frac(forecData,0.8) #select 70% random samples
test.data <- setdiff(forecData,train.data)
forecData
forecData.corr<- cor(forecData,use = "complete.obs")
view(forecData.corr[,c("Value")])
#strating with the most correlated variable as predictor
#Wage would be the the most correlated. but since wage and value are naturally dependend on each other (since the more value, the more wage) and it would be meaningless to predict ones value from the wage we go from the 2nd highest
lm1 <- lm(Value~ShortPassing,data=train.data )
lm1
summary(lm1)
lm2 <- lm(Value~ShortPassing+Volleys,data=train.data)
summary(lm2)
lm3 <- lm(Value~ShortPassing+Volleys+Curve,data=train.data)
summary(lm3)
lm4 <- lm(Value~ShortPassing+Volleys+Curve+Dribbling,data=train.data)
summary(lm4)
lm5 <- lm(Value~ShortPassing+Volleys+Curve+Dribbling+Finishing,data=train.data)
summary(lm5)
#All Data, generally a bad idea due to data noise
lm.tot <- lm(Value~.,data=train.data)
#All Data minus the least important predictors
lm.red <- lm(Value~.-HeightMetric-WeightMetric-Age,data=train.data)
summary(lm.red)
#let's cpompare the models obtained, in a structured way
anova(lm1, lm2, lm3, lm4,lm5, lm.red , lm.tot )
#Set the predictions
pred1 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve), interval="prediction")
pred2 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve), interval="confidence")
pred1.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred1)
pred2.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred2)
pred1.df
pred2.df
pred_test <- predict(lm,data.frame(ShortPassing=80,Volleys=70,Curve= 80), interval="prediction")
# view(pred_test)
pred_test[1]
pred_test <- predict(lm,data.frame(ShortPassing=80,Volleys=70,Curve= 80,Dribbling=100), interval="prediction")
# view(pred_test)
pred_test[1]
#Conlcusion:
#lm.tot is the best but has way too many parameters to be given in our interactive UI and includes the Wage
#lm.red is the same as lm.tot
#lm4 -> sweet spot for best prediction with the least amount of variables
lm <- lm4
#Set the predictions
pred1 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve), interval="prediction")
pred2 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve), interval="confidence")
pred1.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred1)
pred2.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred2)
pred1.df
pred2.df
pred_test <- predict(lm,data.frame(ShortPassing=80,Volleys=70,Curve= 80,Dribbling=100), interval="prediction")
# view(pred_test)
pred_test[1]
pred_test <- predict(lm,data.frame(ShortPassing=80,Volleys=70,Curve= 80,Dribbling=1), interval="prediction")
# view(pred_test)
pred_test[1]
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Value","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve","HeightMetric","WeightMetric")]
count(forecData)
outliers <- boxplot(forecData$Value, plot = TRUE)$out
x <- forecData
forecData <- x[-which(x$Value %in% outliers),]
count(forecData)
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Value","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve","HeightMetric","WeightMetric")]
count(forecData)
outliers <- boxplot(forecData$Value, plot = TRUE)$out
x <- forecData
forecData <- x[-which(x$Value %in% outliers),]
count(forecData)
train.data<- sample_frac(forecData,0.8) #select 70% random samples
test.data <- setdiff(forecData,train.data)
forecData
forecData.corr<- cor(forecData,use = "complete.obs")
view(forecData.corr[,c("Value")])
#strating with the most correlated variable as predictor
#Wage would be the the most correlated. but since wage and value are naturally dependend on each other (since the more value, the more wage) and it would be meaningless to predict ones value from the wage we go from the 2nd highest
lm1 <- lm(Value~ShortPassing,data=train.data )
lm1
summary(lm1)
lm2 <- lm(Value~ShortPassing+Volleys,data=train.data)
summary(lm2)
lm3 <- lm(Value~ShortPassing+Volleys+Curve,data=train.data)
summary(lm3)
lm4 <- lm(Value~ShortPassing+Volleys+Curve+Dribbling,data=train.data)
summary(lm4)
lm5 <- lm(Value~ShortPassing+Volleys+Curve+Dribbling+Finishing,data=train.data)
summary(lm5)
#All Data, generally a bad idea due to data noise
lm.tot <- lm(Value~.,data=train.data)
#All Data minus the least important predictors
lm.red <- lm(Value~.-HeightMetric-WeightMetric-Age,data=train.data)
summary(lm.red)
#let's cpompare the models obtained, in a structured way
anova(lm1, lm2, lm3, lm4,lm5, lm.red , lm.tot )
#Conlcusion:
#lm.tot is the best but has way too many parameters to be given in our interactive UI and includes the Wage
#lm.red is the same as lm.tot
#lm4 -> sweet spot for best prediction with the least amount of variables
lm <- lm4
#Set the predictions
pred1 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve), interval="prediction")
pred2 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve), interval="confidence")
pred1.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred1)
pred2.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred2)
pred1.df
pred2.df
#let's cpompare the models obtained, in a structured way
anova(lm1, lm2, lm3, lm4,lm5, lm.red , lm.tot )
#Conlcusion:
#lm.tot is the best but has way too many parameters to be given in our interactive UI and includes the Wage
#lm.red is the same as lm.tot
#lm4 -> sweet spot for best prediction with the least amount of variables
lm <- lm4
#Set the predictions
pred1 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve), interval="prediction")
#Conlcusion:
#lm.tot is the best but has way too many parameters to be given in our interactive UI and includes the Wage
#lm.red is the same as lm.tot
#lm4 -> sweet spot for best prediction with the least amount of variables
lm <- lm3
#Set the predictions
pred1 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve), interval="prediction")
pred2 <- predict(lm,data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve=test.data$Curve), interval="confidence")
pred1.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred1)
pred2.df <- data.frame(ShortPassing=test.data$ShortPassing,Volleys=test.data$Volleys,Curve= test.data$Curve,Dribbling=test.data$Dribbling,prediction=pred2)
pred1.df
pred2.df
pred_test <- predict(lm,data.frame(ShortPassing=80,Volleys=70,Curve= 80), interval="prediction")
# view(pred_test)
pred_test[1]
pred_test <- predict(lm,data.frame(ShortPassing=100,Volleys=100,Curve=100), interval="prediction")
# view(pred_test)
pred_test[1]
shiny::runApp()
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Value","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve","HeightMetric","WeightMetric")]
##Remove Value = NULL
forecData <- forecData %>% filter(Value!="")
##Remove outliers
outliers <- boxplot(forecData$Value, plot = TRUE)$out
x <- forecData
forecData <- x[-which(x$Value %in% outliers),]
#Train and Test Data
train.data<- sample_frac(forecData,0.7) #select 70% random samples
test.data <- setdiff(forecData,train.data)
#Create Correlation Table
forecData.corr<- cor(forecData,use = "complete.obs")
#View Correlations
ggcorr(forecData.corr,label=TRUE)
#strating with the most correlated variable as predictor
#Wage would be the the most correlated. but since wage and value are naturally dependend on each other (since the more value, the more wage) and it would be meaningless to predict ones value from the wage we go from the 2nd highest
lm1 <- lm(Value~ShortPassing,data=train.data )
summary(lm1)
lm2 <- lm(Value~ShortPassing+Curve,data=train.data)
summary(lm2)
lm3 <- lm(Value~ShortPassing+Curve+Dribbling,data=train.data)
summary(lm3)
lm4 <- lm(Value~ShortPassing+Curve+Dribbling+Volleys,data=train.data)
summary(lm4)
lm5 <- lm(Value~ShortPassing+Curve+Dribbling+Volleys+Crossing,data=train.data)
summary(lm5)
#All Data - this is generally a bad idea due to data noise
lm.tot <- lm(Value~.,data=train.data)
runApp()
shiny::runApp()
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Value","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve","HeightMetric","WeightMetric")]
##Remove Value = NULL
forecData <- forecData %>% filter(Value!="")
##Remove outliers
outliers <- boxplot(forecData$Value, plot = TRUE)$out
x <- forecData
forecData <- x[-which(x$Value %in% outliers),]
#Train and Test Data
train.data<- sample_frac(forecData,0.7) #select 70% random samples
test.data <- setdiff(forecData,train.data)
summary(lm1)
summary(lm2)
summary(lm3)
summary(lm4)
summary(lm.red)
set.seed(69)
forecData <- fifaDataRaw[,c("Age","Value","Crossing","Finishing","HeadingAccuracy","ShortPassing","Volleys","Dribbling","Curve","HeightMetric","WeightMetric")]
##Remove Value = NULL
forecData <- forecData %>% filter(Value!="")
##Remove outliers
outliers <- boxplot(forecData$Value, plot = TRUE)$out
x <- forecData
forecData <- x[-which(x$Value %in% outliers),]
#Train and Test Data
train.data<- sample_frac(forecData,0.7) #select 70% random samples
test.data <- setdiff(forecData,train.data)
#Create Correlation Table
forecData.corr<- cor(forecData,use = "complete.obs")
#View Correlations
ggcorr(forecData.corr,label=TRUE)
#strating with the most correlated variable as predictor
#Wage would be the the most correlated. but since wage and value are naturally
#dependend on each other (since the more value, the more wage)
#and it would be meaningless to predict ones value from the wage we go from the 2nd highest
lm1 <- lm(Value~ShortPassing,data=train.data )
summary(lm1)
lm2 <- lm(Value~ShortPassing+Curve,data=train.data)
summary(lm2)
lm3 <- lm(Value~ShortPassing+Curve+Dribbling,data=train.data)
summary(lm3)
lm4 <- lm(Value~ShortPassing+Curve+Dribbling+Volleys,data=train.data)
summary(lm4)
lm5 <- lm(Value~ShortPassing+Curve+Dribbling+Volleys+Crossing,data=train.data)
summary(lm5)
#All Data - this is generally a bad idea due to data noise
lm.tot <- lm(Value~.,data=train.data)
summary(lm.tot)
#playing around with the least correlated variables and the variable which were a negative coefficient earlier
lm.red <- lm(Value~.-HeightMetric-WeightMetric-Age-Dribbling-Crossing-Finishing,data=train.data)
summary(lm.red)
#let's cpompare the models obtained, in a structured way
anova(lm1, lm2, lm3, lm4,lm5, lm.red , lm.tot)
runApp()
